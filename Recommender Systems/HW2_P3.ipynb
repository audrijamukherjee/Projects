{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb,rgb2hsv\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.sparse.linalg import svds\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block to read in all images, resize, create a NX64X64X3 np array. Also calculate RGB, HSV histograms from original images(not resized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='./Data/'\n",
    "folders=['Animal','Fungus','Geological','Person','plant','Sport']\n",
    "#labels are 0,1,2,3,4,5 respectively\n",
    "#Data is inside ./Data/Folder_Name, Eg, ./Data/Animal\n",
    "\n",
    "##Creating histograms here itself, so I don't have to read in the images again\n",
    "bins1=np.arange(0,257,1) #for RGB the range is 0,255 per channel\n",
    "bins2=np.arange(0,1+(1.0/256),1.0/256)  #for HSV range is 0,1 per channel\n",
    "total=8987\n",
    "RGB_all=np.zeros((total,256*3))\n",
    "HSV_all=np.zeros((total,256*3))\n",
    "\n",
    "#also creating a numpy array of resized images for Autoencoder and SVD\n",
    "num=0\n",
    "for i in range(0,len(folders)):\n",
    "\n",
    "    f=folders[i]\n",
    "    #create list of all image files\n",
    "    temp_list=os.listdir(path+f)\n",
    "    input_temp2=[]\n",
    "    for each in temp_list:\n",
    "        img = imread(path+f+r'/'+each)  #original image is 0-255\n",
    "        if len(img.shape) == 2:\n",
    "            img= gray2rgb(img)     #convert to RGB if in grayscale\n",
    "\n",
    "        input_temp2.append(resize(img, (64,64,3)))  #64X64X3 resized images\n",
    "        #RGB histogram, using original images\n",
    "        single_hist1 = np.zeros((3, 256))\n",
    "        for ch in range(0, 3):\n",
    "            # 3 channels R,G,B, original image is 0-255\n",
    "            single_hist1[ch], _ = np.histogram(img, bins=bins1)\n",
    "        RGB_all[num] = np.concatenate((single_hist1[0], single_hist1[1], single_hist1[2]), axis=0)\n",
    "\n",
    "        #HSV histogram\n",
    "        img_hsv=rgb2hsv(img)\n",
    "        single_hist2 = np.zeros((3, 256))\n",
    "        for ch in range(0, 3):\n",
    "            # 3 channels H,S,V\n",
    "            single_hist2[ch], _ = np.histogram(img_hsv, bins=bins2)\n",
    "        HSV_all[num] = np.concatenate((single_hist2[0], single_hist2[1], single_hist2[2]), axis=0)\n",
    "        num += 1\n",
    "    if i==0:\n",
    "        input_resized=np.array(input_temp2)\n",
    "        labels=np.zeros(len(temp_list))\n",
    "        labels.fill(i)\n",
    "    else:\n",
    "        input_resized=np.concatenate((input_resized,np.array(input_temp2)),axis=0)\n",
    "        temp=np.zeros(len(temp_list))\n",
    "        temp.fill(i)\n",
    "        labels=np.concatenate((labels,temp),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test, train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting the dataset into train and test data\n",
    "#total=8987, making train set of 7190 and test set of 1797\n",
    "#generate 0.2*total random indices\n",
    "\n",
    "total=input_resized.shape[0]\n",
    "\n",
    "#train and test sets for Autoencoder, SVD of resized images\n",
    "indices=np.random.permutation(total)\n",
    "indices_train=indices[:7190]\n",
    "indices_test=indices[7190:]\n",
    "x_train=input_resized[indices_train]\n",
    "labels_train=labels[indices_train]\n",
    "x_test=input_resized[indices_test]\n",
    "labels_test=labels[indices_test]\n",
    "total_train=x_train.shape[0]\n",
    "total_test=x_test.shape[0]\n",
    "\n",
    "#also split the RGB, HSV histogram vectors by the same indices\n",
    "RGB_train=RGB_all[indices_train]\n",
    "RGB_test=RGB_all[indices_test]\n",
    "HSV_train=HSV_all[indices_train]\n",
    "HSV_test=HSV_all[indices_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common function for Euclidean and Correlation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##KNN code\n",
    "\n",
    "#For Euclidean distance: call with algorithm='ball_tree',metric='euclidean'\n",
    "#For Pearson Coefficient: call with algorithm='brute', metric='correlation')\n",
    "\n",
    "def KNN_5(test_set,train_set,labels_train,algorithm, metric):\n",
    "    # for each image in test set, find the closest 5 from train set\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm=algorithm,metric=metric).fit(train_set)\n",
    "    distances, indices = nbrs.kneighbors(test_set)\n",
    "    pred=np.zeros(len(test_set))\n",
    "    #get the 5 corresponding labels, choose majority\n",
    "    for i in range(0,len(indices)):\n",
    "        pred[i]=mode(labels_train[indices[i]])[0][0] #list of labels of 5 NN\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert input images to vectors: Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "input_img = Input(shape=(64, 64, 3))  \n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "#16X16X8\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "encoder=Model(input_img,encoded)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy',metrics=['mse'])\n",
    "\n",
    "autoencoder.fit(x_train,x_train, \n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test)) #CHANGE\n",
    "\n",
    "#Using resized images as input\n",
    "Auto_train_temp=encoder.predict(x_train)\n",
    "Auto_test_temp=encoder.predict(x_test)\n",
    "\n",
    "#Flatten both vectors\n",
    "Auto_train=np.zeros((total_train,16*16*8))\n",
    "for i in range(0,total_train):\n",
    "    Auto_train[i]=np.ravel(Auto_train_temp[i])\n",
    "\n",
    "Auto_test=np.zeros((total_test,16*16*8))\n",
    "for i in range(0,total_test):\n",
    "    Auto_test[i]=np.ravel(Auto_test_temp[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Autoencoder architecture details:\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 64, 64, 8)         224       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 32, 32, 8)         584       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 16, 16, 8)         584       \n",
    "_________________________________________________________________\n",
    "up_sampling2d_1 (UpSampling2 (None, 32, 32, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 32, 32, 8)         584       \n",
    "_________________________________________________________________\n",
    "up_sampling2d_2 (UpSampling2 (None, 64, 64, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 64, 64, 3)         219       \n",
    "================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reduced the number of filters per layer to get a more reduced length vector. The feature length per image is now 16*16*8=2048. The The blog used for Part 2 of the homework used grayscale images as input. To make the architecture work for RGB images, I tweaked the decoder half to produce 64X64X3 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert input images to vectors: SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert images to vector by SVD\n",
    "#Using resized images as input\n",
    "k= 256#number of sv to retain\n",
    "\n",
    "svd_input_train=np.zeros((total_train,64*64*3))\n",
    "for i in range(0,total_train):\n",
    "    svd_input_train[i]=np.ravel(x_train[i])\n",
    "svd_input_test = np.zeros((total_test, 64 * 64 * 3))\n",
    "for i in range(0, total_test):\n",
    "    svd_input_test[i] = np.ravel(x_test[i])\n",
    "svd = TruncatedSVD(n_components=k)\n",
    "svd.fit(svd_input_train)\n",
    "SVD_train=svd.transform(svd_input_train)\n",
    "SVD_test=svd.transform(svd_input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also tried, scipy.sparse.linalg.svds. Calculated vectors by svds, then split into test and train sets. It gave slightly lower accuracy than using TruncatedSVD. Hence, using TruncatedSVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method to plot confusion matrix\n",
    "def plot_cm(conf_matrix):\n",
    "    folders=['Animal','Fungus','Geological','Person','plant','Sport']\n",
    "    df = pd.DataFrame(conf_matrix, index =folders,columns = folders)\n",
    "    plt.pcolor(df)\n",
    "    plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "    plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Autoencoder - Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Call for predicting, Confusion matrix calculation and accuracy calculation\n",
    "\n",
    "print (\"Results for Autoencoder- KNN+Euclidean distance\")\n",
    "knn_pred_Autoencoder=KNN_5(Auto_test,Auto_train, labels_train,algorithm='ball_tree',metric='euclidean')\n",
    "print (\"Confusion Matrix \")\n",
    "conf_matrix=confusion_matrix(labels_test, knn_pred_Autoencoder)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,knn_pred_Autoencoder))\n",
    "plot_cm(conf_matrix)\n",
    "\n",
    "print(\"Results for Autoencoder- KNN+Pearson Correlation\")\n",
    "pear_pred_Autoencoder=KNN_5(Auto_test,Auto_train, labels_train,algorithm='brute', metric='correlation')\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, pear_pred_Autoencoder)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,pear_pred_Autoencoder))\n",
    "plot_cm(conf_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for Autoencoder- KNN+Euclidean distance\n",
    "\n",
    "Confusion Matrix \n",
    "[[107  26 123  13  43  20]\n",
    " [ 63  60  39   7  62  10]\n",
    " [ 59  14 262   9  15  15]\n",
    " [ 57  23  97  31  17   9]\n",
    " [ 61  43  62   8  71   8]\n",
    " [ 99  22 125   2  44  71]]\n",
    "ACCURACY =  0.335002782415\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf1.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for Autoencoder- KNN+Pearson Correlation\n",
    "Confusion Matrix \n",
    "[[112  33 104  22  43  18]\n",
    " [ 40  75  32  13  75   6]\n",
    " [ 47  21 252  18  16  20]\n",
    " [ 42  26  80  62  18   6]\n",
    " [ 46  44  55  12  87   9]\n",
    " [ 98  23 108   9  44  81]]\n",
    "ACCURACY =  0.372287145242"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Autoencoder reduced vector accuracy is ~33.5% when using 5-NN with Euclidean distance and ~37% when using Pearson coefficient. Accuracy using the 2 metrics is comparable. From the confusion matrix, we can see that Geological category is prediced well by this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " SVD - Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results for SVD- KNN+Euclidean distance\")\n",
    "print (\"Results for SVD\")\n",
    "knn_pred_SVD=KNN_5(SVD_test,SVD_train, labels_train,algorithm='ball_tree',metric='euclidean')\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, knn_pred_SVD)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,knn_pred_SVD))\n",
    "plot_cm(conf_matrix)\n",
    "\n",
    "print(\"Results for SVD- KNN+Pearson Correlation\")\n",
    "pear_pred_SVD=KNN_5(SVD_test,SVD_train, labels_train,algorithm='brute', metric='correlation')\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, pear_pred_SVD)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,pear_pred_SVD))\n",
    "plot_cm(conf_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for SVD- KNN+Euclidean distance\n",
    "Confusion Matrix \n",
    "[[113  16 124   8  39  17]\n",
    " [ 76  69  70   3  22   7]\n",
    " [ 38   5 287   4  12  11]\n",
    " [ 70  25  92  32   9   9]\n",
    " [ 65  34  80   7  82   5]\n",
    " [104  11 166   7  19  59]]\n",
    "ACCURACY =  0.357262103506\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf3.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for SVD- KNN+Pearson Correlation\n",
    "Confusion Matrix \n",
    "[[130   9 122  14  26  16]\n",
    " [ 59  65  81   7  29   6]\n",
    " [ 39   6 291   7   8   6]\n",
    " [ 60  16  98  43   6  14]\n",
    " [ 67  27  73   9  91   6]\n",
    " [ 85   8 179  11  23  60]]\n",
    "ACCURACY =  0.378408458542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried small k values such as 10, 25, etc. As expected the accuracy is very low for smaller k values. I chose 256 because it gave good results and a k value much higher than 256 takes longer to run and didn't seem to improve accuracy noticeably. For SVD reduced vector, for feature length 256, accuracy is ~35.5% when using 5-NN with Euclidean distance and ~38% when using Pearson coefficient. Accuracy using the 2 metrics is comparable. It is slightly higher than that for Autoencoder reduced images. From the confusion matrix, we can see that Geological category is prediced well by this method also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RGB - Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results for RGB Histogram- KNN+Euclidean distance\")\n",
    "knn_pred_RGB=KNN_5(RGB_test,RGB_train, labels_train,algorithm='ball_tree',metric='euclidean')\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, knn_pred_RGB)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,knn_pred_RGB))\n",
    "plot_cm(conf_matrix)\n",
    "       \n",
    "print(\"Results for RGB Histogram- Pearson Correlation\")\n",
    "pear_pred_RGB=KNN_5(RGB_test,RGB_train, labels_train,algorithm='brute', metric='correlation')  #CHANGE\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, pear_pred_RGB)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,pear_pred_RGB))\n",
    "plot_cm(conf_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for RGB Histogram- KNN+Euclidean distance\n",
    "Confusion Matrix \n",
    "[[104  60  67  25  36  25]\n",
    " [ 52 121  19  13  41   8]\n",
    " [100  39 139  12  23  40]\n",
    " [ 44  61  31  65  28  13]\n",
    " [ 47  60  24  29  68   7]\n",
    " [ 92  40  87  17  33 127]]\n",
    "ACCURACY =  0.347245409015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf5_2.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for RGB Histogram- Pearson Correlation\n",
    "Confusion Matrix \n",
    "[[ 99  62  61  26  37  24]\n",
    " [ 50 121  21  26  30   7]\n",
    " [101  58 127  17  30  30]\n",
    " [ 56  72  33  68  24  17]\n",
    " [ 63  62  39  23  59   8]\n",
    " [ 95  44  67  24  31  85]]\n",
    "ACCURACY =  0.311074012243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RGB histogram reduced vector, feature length is 256*3, 256 bins per channel (R,G,B) of the original images. Accuracy is ~34.5% when using 5-NN with Euclidean distance and ~31% when using Pearson coefficient. Accuracy using the 2 metrics is comparable. From the confusion matrix, we can see that the predictions are more balanced for the categories as compared to those of Autoencoder, SVD predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " HSV - Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results for HSV Histogram- KNN+Euclidean distance\")  \n",
    "knn_pred_HSV=KNN_5(HSV_test,HSV_train, labels_train,algorithm='ball_tree',metric='euclidean')\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, knn_pred_HSV)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,knn_pred_HSV))\n",
    "plot_cm(conf_matrix)\n",
    "       \n",
    "print(\"Results for HSV Histogram- KNN+Pearson Correlation\")\n",
    "pear_pred_HSV=KNN_5(HSV_test,HSV_train, labels_train,algorithm='brute', metric='correlation')\n",
    "print (\"Confusion Matrix )\n",
    "conf_matrix=confusion_matrix(labels_test, pear_pred_HSV)\n",
    "print (conf_matrix)\n",
    "print (\"ACCURACY = \",accuracy_score(labels_test,pear_pred_HSV))\n",
    "plot_cm(conf_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for HSV Histogram- KNN+Euclidean distance\n",
    "Confusion Matrix \n",
    "[[107  87  45  17  36  26]\n",
    " [ 52 124  22  13  18  10]\n",
    " [ 60  74 132  12  18  64]\n",
    " [ 39  61  24  75   8  28]\n",
    " [ 36  57  11  12 145   8]\n",
    " [ 52  72  47  17  21 167]]\n",
    "ACCURACY =  0.417362270451"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf7.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for HSV Histogram- KNN+Pearson Correlation\n",
    "Confusion Matrix \n",
    "[[117  77  48  21  34  15]\n",
    " [ 50 137  14   5  23   7]\n",
    " [ 67  78 143  23  18  27]\n",
    " [ 44  63  29  81  10  18]\n",
    " [ 54  47  23  11 147   6]\n",
    " [ 58  81  57  31  30 103]]\n",
    "ACCURACY =  0.405119643851"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cf8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HSV histogram reduced vector, I used feature length 256*3, 256 bins per channel (H,S,V). Accuracy is ~42% when using 5-NN with Euclidean distance and ~40.5% when using Pearson coefficient. From the confusion matrix, we can see that the predictions are more balanced for the categories as compared to those of Autoencoder, SVD predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy is highest for HSV Histogram vectors, for 5-NN using Euclidean distance. The next better one would be SVD reduced vectors for this autoencoder architecture and dataset. Although confusion matrix reflects more balanced predictions for HSV and RGB histograms than for Autoencoder and SVD reduced images. All accuracy values range from 31%-42%. Also, for Autoencoder, SVD Pearson coefficient seems to work better, whereas fro RGB, HSV histograms, predictions using Euclidean distance are better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}